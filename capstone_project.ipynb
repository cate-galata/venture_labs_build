{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project Podcast Profiler\n",
    "\n",
    "This is the hands-on session for the capstone project of the workshop.\n",
    "\n",
    "Copyright (c) 2023 Michael Neumayr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Set up the Colab in your drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load this Colab from Github\n",
    "- Run the first cell to install all required packages (this takes a moment)\n",
    "- During installation jump to section \"Set OpenAI API Key\" and put the key we provide you instead of \"PUT_YOUR_KEY_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required packages; this may take some minutes; ignore dependency warnings it should work anyway\n",
    "%pip install openai\n",
    "%pip install langchain\n",
    "%pip install pypdf\n",
    "%pip install tiktoken\n",
    "%pip install pydub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the workshop github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/michaelnoi/venture_labs_build.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd venture_labs_build\n",
    "!git checkout only_static_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY', 'PUT_YOUR_KEY_HERE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Optional: Connect to your Google Drive storage to upload your own documents later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to your google drive storage\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project: Podcast Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the podcast episodes as audio files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give you three podcast episodes from the Venture Labs' [Entrepreneurial Realities](https://open.spotify.com/show/1r0vpcIZomm9W7ozp3qWLV) podcast. We have three episodes with the titles being the names of the guests. Below you see the episodes and the respective .mp3 file size, which will be important when we will transcribe them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./static/podcast_episode.png\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load an episode into the Colab:. As our transcription API will have a limit of 25 MB let's load the first that doesn't hit this limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"rb\" option is important to (r)ead the file in (b)inary format\n",
    "audio_file_alexa = open(\"./static/Alexa Sinyachova.mp3\", \"rb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transcribe the podcast episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for transcription we use the Audio part of the openai library\n",
    "# running this may take a bit since we are uploading and processing half an hour of audio\n",
    "episode_alexa = openai.Audio.transcribe(\"whisper-1\", audio_file_alexa, api_key=openai_api_key)\n",
    "episode_alexa = episode_alexa[\"text\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(episode_alexa)\n",
    "\n",
    "# load an LLM just see how many tokens are in the file\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)\n",
    "\n",
    "print(\"Number of tokens:\", llm.get_num_tokens(episode_alexa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the transcription of the first episode. Watch out that the transcription exceeds the token limit of the language models we used to summarize text. We will need to split up the text later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color:#151E35; color:#A450E6\">\n",
    "    ðŸŽ¯ <b>TODO</b>\n",
    "  <p>Summarize the episode with <b>Tom Eisenmann</b> on your own now and store it into the variable <code style=\"color:#A450E6\">episode_tom</code>! Don't summarize the episode with <b>Michael Eckhardt</b> yet as it will hit the upload limit of the Whisper API.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: open the episode\n",
    "audio_file_tom = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: do the transcription\n",
    "episode_tom = ...\n",
    "episode_tom = episode_tom[\"text\"]\n",
    "\n",
    "\n",
    "# close the file again so that we free up system resources\n",
    "audio_file_tom.close()\n",
    "print(\"Number of tokens:\", llm.get_num_tokens(episode_tom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Handle too large audio file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! For the episode with Michael Eckhardt, we need to find a different solution since the upload limit for the whisper API is 25 MB and the episode has almost 30 MB. \n",
    "\n",
    "Let's try to split the episode into two parts and transcirbe them separately. Then combine the strings again to get the full episode. Let's see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use this package to segment our audio file into two parts and store them separately\n",
    "from pydub import AudioSegment\n",
    "\n",
    "episode_michael = AudioSegment.from_mp3(\"./static/Michael Eckhardt.mp3\")\n",
    "\n",
    "# PyDub handles time in milliseconds\n",
    "fifteen_minutes = 15 * 60 * 1000\n",
    "\n",
    "first_part_episode_michael = episode_michael[:fifteen_minutes]\n",
    "second_part_episode_michael = episode_michael[fifteen_minutes:]\n",
    "\n",
    "# save both parts of the episode into the static/ folder\n",
    "first_part_episode_michael.export(\"./static/Michael_Eckhardt_1.mp3\", format=\"mp3\")\n",
    "second_part_episode_michael.export(\"./static/Michael_Eckhardt_2.mp3\", format=\"mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color:#151E35; color:#A450E6\">\n",
    "    ðŸŽ¯ <b>TODO</b>\n",
    "  <p>Ok, now transcribe the both parts and concatenate the transcriptions to have the full episode.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: load both parts of the episode\n",
    "audio_first_part = ...\n",
    "audio_second_part = ...\n",
    "\n",
    "### TODO: do the transcription\n",
    "first_part_episode_michael = ...\n",
    "second_part_episode_michael = ...\n",
    "\n",
    "# close the files again so that we free up system resources\n",
    "audio_first_part.close()\n",
    "audio_second_part.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the two parts and see how many tokens we have this time\n",
    "episode_michael =  first_part_episode_michael[\"text\"] + second_part_episode_michael[\"text\"]\n",
    "\n",
    "print(\"Number of tokens:\", llm.get_num_tokens(episode_michael))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Split the transcribed podcast episodes into chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the TODOs from here refer back to the text_summarization.ipynb notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color:#151E35; color:#A450E6\">\n",
    "    ðŸŽ¯ <b>TODO</b>\n",
    "  <p>Split all three episodes into chunks of size 8000 (characters ~ 2000 tokens ~ 1500 words) with an overlap of 500 characters. </p>\n",
    "  <p>Go back to the summarization notebook if you need to.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: initialize the text splitter\n",
    "text_splitter = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: split each episode string into chunks with the splitter\n",
    "chunks_alexa = text_splitter.create_documents([...])\n",
    "chunks_tom = ...\n",
    "chunks_michael = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Alexa's episode has {len(chunks_alexa)} chunks.\")\n",
    "print(f\"Tom's episode has {len(chunks_tom)} chunks.\")\n",
    "print(f\"Michael's episode has {len(chunks_michael)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Summarize the podcast episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# define our llm for summarization\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color:#151E35; color:#A450E6\">\n",
    "    ðŸŽ¯ <b>TODO</b>\n",
    "  <p>Now summarize all three chunked episodes with the map_reduce technique with the pre-defined chain like in the summarization hands-on.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: initialize the summarization chain as in the summarization hands-on\n",
    "summarizer = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: summarize each episode, remember that the .create_documents() needs the chunks in a list as input: [chunks]\n",
    "summary_alexa = ...\n",
    "summary_tom = ...\n",
    "summary_michael = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to get the output_text. It's sometimes helpful to strip the text of leading and trailing whitespace.\n",
    "print(summary_alexa[\"output_text\"].strip())\n",
    "print(summary_tom[\"output_text\"].strip())\n",
    "print(summary_michael[\"output_text\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Make a cool Podcast profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color:#151E35; color:#A450E6\">\n",
    "    ðŸŽ¯ <b>TODO</b>\n",
    "  <p>Design some smart prompts to get more information (featured speakers, topic, tags, mood, etc.) from the summary or the first chunk (beginning of the podcast) as data and then build a cool podcast profile.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: make interesting prompts for the podcast episode profile\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
