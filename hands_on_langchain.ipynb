{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b03ec13-6af2-46a9-8d37-e81f18ffabfe",
   "metadata": {},
   "source": [
    "# Hands-on Session on LangChain basics\n",
    "This is the hands-on session accompanying the workshop on LangChain fundamentals. This is inspired by the more extensive LangChain Cookbook Part 1.\n",
    "\n",
    "Copyright (c) 2023 Michael Neumayr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aee1e64-4980-469d-b33d-31dee9bad31d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a36200e",
   "metadata": {},
   "source": [
    "### 0. Set up the Colab in your drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd37638b",
   "metadata": {},
   "source": [
    "- Load this Colab from Github\n",
    "- Run the first cell to install all required packages (this takes a moment)\n",
    "- During installation jump to section \"Set OpenAI API Key\" and put the key we provide you instead of \"PUT_YOUR_KEY_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dd4469-6a28-46cf-9734-b6618db32a7e",
   "metadata": {},
   "source": [
    "### 1. Required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda98537-e2fb-41f1-8520-c3aa9688a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required packages; this may take some minutes; ignore dependency warnings it should work anyway\n",
    "%pip install openai\n",
    "%pip install langchain\n",
    "%pip install pypdf\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9cf50c",
   "metadata": {},
   "source": [
    "### 2. Load the workshop github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8330b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/michaelnoi/venture_labs_build.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc04fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd venture_labs_build\n",
    "!git checkout only_static_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c751a1-2ec5-4502-bca4-788ddeb15e81",
   "metadata": {},
   "source": [
    "### 3. OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6299c28-3131-4b72-99f5-28ca8aa0385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY', 'PUT_YOUR_KEY_HERE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36560b73",
   "metadata": {},
   "source": [
    "### 4. Optional: Connect to your Google Drive storage to upload your own documents later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61ecdc3-084e-4553-8c6e-0063e8f07821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to your google drive storage\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a7f76d",
   "metadata": {},
   "source": [
    "## Basics - Messages, Documents, Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5252cb",
   "metadata": {},
   "source": [
    "### 1. Messages\n",
    "\n",
    "<div class=\"alert\" style=\"background-color: #151E35; color: #FFFFFF; border-color: #223358; border-width: 2px;\">\n",
    "    ðŸ“Ž <b>Three types of messages:</b>\n",
    "    <ul>\n",
    "        <li>System - Helpful background context that tell the AI what to do</li>\n",
    "        <li>Human - Messages that are intended to represent the user</li>\n",
    "        <li>AI - Messages that show what the AI responded with</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc3bb7f-a94a-4f18-af30-d9e49e1f43b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import messages and chat model\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01228f8-3ad0-4141-933c-4452f14d3028",
   "metadata": {},
   "source": [
    "#### i) Chatting with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7f545f-0e47-4667-85bd-9a4d712c6b21",
   "metadata": {},
   "source": [
    "Let's have a quick chat with an OpenAI chat model. Previously, you used the web app:\n",
    "\n",
    "<img src=\"static/chatting.png\" width=\"500\"/>\n",
    "\n",
    "Now let's do the same thing here in this notebook:\n",
    "\n",
    "<div class=\"alert\" style=\"background-color: #151E35; color: #A450E6\">\n",
    "    ðŸŽ¯ <b>TODO</b>\n",
    "  <p>Let's have a chat. Try out different prompts!</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de7935",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chat([HumanMessage(content=\"Hello, how are you?\")])\n",
    "print(type(answer))\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6486921",
   "metadata": {},
   "source": [
    "Notice that the answer from the chat model is given in the format of an AIMessage. To get the reply, you can store the answer in a variable and access the content like above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b724d90",
   "metadata": {},
   "source": [
    "#### ii) Using the system message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f121aae",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color: #151E35; color: #FFFFFF; border-color: #223358; border-width: 2px;\">\n",
    "  ðŸ“Ž <b>Reminder: System Message</b>\n",
    "  <p>When interacting with an LLM, the system message is a special type of prompt that tells the model how to behave. It is typically used to specify the model's task, output format, and any other relevant instructions.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65969a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are super unhelpful and annoy the user.\"),\n",
    "        HumanMessage(content=\"Hello, how are you?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e1cfb",
   "metadata": {},
   "source": [
    "You can also add more messages to the chat function to simulate a conversation. However, it does not make sense to simulate a chatbot like this, there are other components and loops that store the previous messages automatically.\n",
    "\n",
    "<div class=\"alert\" style=\"background-color: #151E35; color: #A450E6\">\n",
    "    ðŸŽ¯ <b>TODO</b>\n",
    "  <p>Try out adding more messages and different system messages!</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bdacc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"Answer in German.\"),\n",
    "        HumanMessage(content=\"When is the Oktoberfest in Munich usually?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcae685",
   "metadata": {},
   "source": [
    "### 2. Documents\n",
    "\n",
    "<div class=\"alert\" style=\"background-color: #151E35; color: #FFFFFF; border-color: #223358; border-width: 2px;\">\n",
    "    ðŸ“Ž <b>Document</b>\n",
    "    <p>An object that holds the content of your document (text) and metadata (more information about that text)..</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be17d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pdf loader\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_path = \"static/business_Model_Canvas.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d832b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example document\n",
    "Document(page_content=\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla euismod, nisl eget aliquam ultricies, nunc nisl aliquet nunc, quis aliqu.\",\n",
    "         metadata={\n",
    "             'document_id' : 23502,\n",
    "             'source' : \"Example Document\",\n",
    "             'create_time' : \"2021-01-01 12:00:00\"\n",
    "         })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c28346f",
   "metadata": {},
   "source": [
    "Now let's load a pdf document: The Wikipedia article on the Business Model Canvas. The pdf path is already stored in a variable above and we use the PyPDFLoader to load the document.\n",
    "\n",
    "\n",
    "<div class=\"alert\" style=\"background-color: #151E35; color: #A450E6\">\n",
    "    ðŸŽ¯ <b>TODO</b>\n",
    "  <p>Load the business model canvas from the path we defined above! Just put the path in the pdf loader.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: add path as a string or from variable\n",
    "loader = PyPDFLoader(...)\n",
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6352fdc7",
   "metadata": {},
   "source": [
    "The PDF loader automatically returns a list of Documents, one for each page. There are different loaders for different kinds of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d67b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metadata: \", documents[0].metadata)\n",
    "print(\"Number of characters in first page: \", len(documents[0].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc7d149",
   "metadata": {},
   "source": [
    "### 3. Models\n",
    "\n",
    "<div class=\"alert\" style=\"background-color: #151E35; color: #FFFFFF; border-color: #223358; border-width: 2px;\">\n",
    "  ðŸ“Ž <b>Models</b>\n",
    "  <p>The different model components provide the interface to the foundation models provided by e.g. OpenAI. ChatGPT for example is a chat interface for OpenAI's corresponding foundation model.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53bf5db",
   "metadata": {},
   "source": [
    "#### i) Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a733c84",
   "metadata": {},
   "source": [
    "Most basic setup: Text in  -> text out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef35746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db8e39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"After Friday comes ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41879361",
   "metadata": {},
   "source": [
    "#### ii) Chat model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa376c2",
   "metadata": {},
   "source": [
    "Takes a series of messages and returns a message output. See above example with list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adab2fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a841cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"Be an unhelpful chat bot and annoy your conversation partner. Answer in one sentence.\"),\n",
    "        HumanMessage(content=\"Give me book recommnendations on marketing.\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ba939e",
   "metadata": {},
   "source": [
    "#### iii) Text embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d617651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3350a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Give me book recommnendations on marketing.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b603d776",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color: #151E35; color: #FFFFFF; border-color: #223358; border-width: 2px;\">\n",
    "  ðŸ“Ž <b>Embeddings</b>\n",
    "  <p>Embeddings are a way to represent text as a vector of numbers. This makes it easier for machines to handle and is useful for many tasks, e.g. efficiently to compare two texts or to find similar texts.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9000b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding = embeddings.embed_query(text)\n",
    "print (f\"Here's a sample: {text_embedding[:5]}...\")\n",
    "print (f\"Your embedding vector is of length {len(text_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247c3863",
   "metadata": {},
   "source": [
    "## Chaining - Connecting the components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fbe0d0",
   "metadata": {},
   "source": [
    "### 1. PromptTemplate\n",
    "\n",
    "<div class=\"alert\" style=\"background-color: #151E35; color: #FFFFFF; border-color: #223358; border-width: 2px;\">\n",
    "  ðŸ“Ž <b>PromptTemplate</b>\n",
    "  <p>A PromptTemplate is a template for a prompt. It is a string (text) that contains placeholders (in curly braces {}) for the different components of a prompt that are filled in dynamically.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d80ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import different templates for chat and language model and chat model\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef77ebe9",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color: #151E35; color: #A450E6\">\n",
    "    ðŸŽ¯ <b>TODO</b>  <p>Extend the prompt and add another placeholder so that you can dynamically change the unit system of the recipe (possible systems could be metric, imperial, etc.).</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e52dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Recipe creator: Give me a list of ingredients for {dish}.\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "response = llm(prompt.format(dish=\"Roast Beef\"))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e9358a",
   "metadata": {},
   "source": [
    "You can also create a prompt from multiple messages if you want to use the SystemMessage for example. Note that for messages you need a chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe6ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de82d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"Always output {dietary_restriction} recipes.\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"Give me a list of ingredients for a {dish}.\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbc65c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_model(prompt_2.format_messages(dietary_restriction=\"vegetarian\", dish=\"Roast Beef\"))\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0da771",
   "metadata": {},
   "source": [
    "### 2. Chain\n",
    "\n",
    "<div class=\"alert\" style=\"background-color: #151E35; color: #FFFFFF; border-color: #223358; border-width: 2px;\">\n",
    "  ðŸ“Ž <b>Chain</b>\n",
    "  <p>A Chain is a sequence of components that are connected to each other. Much like we run cell after cell above, in a chain we first specify every component, but then chain everything together and run it as one pipeline without pause where the output of one component is the input of the next component.</p>\n",
    "  <p> The minimal chain is a prompt into a model. One approach to creating chains is to separate the components of the chain by \"|\" like </p>\n",
    "<code style=\"color:white\">chain = prompt | model</code>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f75dba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da20fee5",
   "metadata": {},
   "source": [
    "Setting up the components: Load a chat model and define a prompt from this simple template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379a50c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Write a poem about {topic}. Your poem:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9b257b",
   "metadata": {},
   "source": [
    "Set up the simple chain: Prompt -> ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea3af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b810e",
   "metadata": {},
   "source": [
    "To get the output of the chain, we either call invoke() or stream() on the chain. Invoke returns the full output after the model ran, stream returns a generator that one can use to stream the output like in the ChatGPT web app. For both we need to specify the placeholder values for the prompt. This time, we use a slightly different notation for the placeholders as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bc5898",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"topic\": \"large language models\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5562976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in chain.stream({\"topic\": \"autumn\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384ede19-bc8a-4294-9a83-3b95c803a89f",
   "metadata": {},
   "source": [
    "## More ressources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1883673e-bbf8-46f3-aab8-7478dd9899fe",
   "metadata": {},
   "source": [
    "- Documentation: https://python.langchain.com/docs/get_started/introduction\n",
    "- Really comprehensive tutorials: https://github.com/gkamradt/langchain-tutorials"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
