{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Session Document Q&A\n",
    "This is the hands-on session accompanying the workshop on LangChain fundamentals. This is inspired by the more extensive LangChain Cookbook Part 1.\n",
    "\n",
    "Copyright (c) 2023 Michael Neumayr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Set up the Colab in your drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load this Colab from Github\n",
    "- Run the first cell to install all required packages (this takes a moment)\n",
    "- During installation jump to section \"Set OpenAI API Key\" and put the key we provide you instead of \"PUT_YOUR_KEY_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required packages; this may take some minutes; ignore dependency warnings it should work anyway\n",
    "%pip install openai\n",
    "%pip install langchain\n",
    "%pip install pypdf\n",
    "%pip install tiktoken\n",
    "%pip install chromadb\n",
    "%pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the workshop github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/michaelnoi/venture_labs_build.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd venture_labs_build\n",
    "!git checkout only_static_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY', 'PUT_YOUR_KEY_HERE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Optional: Connect to your Google Drive storage to upload your own documents later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to your google drive storage to use your own documents\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Document Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"static/rag.jpeg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is RAG?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG is a technique for augmenting LLM knowledge with additional data.\n",
    "\n",
    "LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model’s cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).\n",
    "\n",
    "RAG is made up of two components: indexing and retrieval+generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first component of RAG requires ingesting data from a source and indexing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load a document (PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# load short business model canvas pdf again\n",
    "pdf_path = \"static/natural_language_processing.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the natural language processing wikipedia again as pdf document. We know this document has around 6k tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall number of tokens: 6650\n"
     ]
    }
   ],
   "source": [
    "overall_tokens = 0\n",
    "for page in documents[:-4]:\n",
    "    n_tokens = llm.get_num_tokens(page.page_content)\n",
    "    overall_tokens += n_tokens\n",
    "    \n",
    "print(f\"Overall number of tokens: {overall_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Split the document into chunks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won’t fit in a model’s finite context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import predefined chain for text splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a text splitter to split up our document into chunks. This time the chunks must contain fewer tokens since we will be feeding more than one in the same context window of our LLM Q&A chatbot later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 26\n",
      "Number of tokens in each chunk:\n",
      "295\n",
      "317\n",
      "378\n",
      "301\n",
      "299\n",
      "331\n",
      "76\n",
      "312\n",
      "287\n",
      "98\n",
      "281\n",
      "335\n",
      "251\n",
      "281\n",
      "312\n",
      "274\n",
      "294\n",
      "300\n",
      "262\n",
      "120\n",
      "323\n",
      "320\n",
      "302\n",
      "318\n",
      "316\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "\n",
    "# put relevant pages into one string\n",
    "article = \"\"\n",
    "for page in documents[:-4]:\n",
    "    article += page.page_content + \"\\n\\n\"\n",
    "\n",
    "# split into chunks with the defined text splitter\n",
    "chunks = text_splitter.create_documents([article])\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(\"Number of tokens in each chunk:\")\n",
    "for chunk in chunks:\n",
    "    print(llm.get_num_tokens(chunk.page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Embeddings and Vectorstores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings create a vector (numerical) representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space. The numerical representation of the text chunks can be used to mathematicaly commpare documents: similar documents will be closer in the vector space than different documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector store takes care of storing embedded data and performing vector search for you. At query time we embed the user query (Question) and retrieve the embedding vectors (of the text chunks) that are 'most similar' to the embedded query.\n",
    "\n",
    "The vectorstore we use for this exercise is Chroma because it is in-memory, which makes it very easy to use. LangChain offers integrations with over 30 vectorstores, some of which are more suited for storing large amounts of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to store the vectorstore so that we can use it later on\n",
    "persist_directory = 'docs/chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./docs/chroma  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vectorstore\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "# should be the same as the number of text chunks from before \n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieval and Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual RAG chain is composed of retrieval and generation: the user queries the document at run time, the relevant data is retrieved from the index and passed to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a user input, relevant splits are retrieved from storage using a Retriever. \n",
    "\n",
    "Similarity search simply retrives the k most similar text chunk embeddings to our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Illustration of the field by a brain and\\na microchip interacting via language,\\nknowledge representation, signal\\nprocessing, programming etc.Natural language processing\\nNatural language processing (NLP) is an interdisciplinary subfield of computer\\nscience and linguistics. It is primarily concerned with giving computers the ability to\\nsuppor t and manipulate speech. It involves processing natural langua ge datasets,\\nsuch as text corpora or speech corpora, using either rule-based or probabilistic (i.e.\\nstatistical and, most recently, neural network-based) machine learning approaches.\\nThe goal is a computer capable of \"unde rstanding\" the contents of documents,\\nincluding the contextual nuances of the langua ge within them. The technology can\\nthen accurately extract information and insights contained in the documents as well\\nas categorize and or ganize the documents themselves.\\nChallenges in natural langua ge processing frequently involve speech recognition,\\nnatural-langua ge unde rstanding, and natural-langua ge generation.\\nNatural langua ge processing has its roots in the 1950s . Already in 1950, Alan Turing published an article titled \"Computing\\nMachinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the\\ntime that was not articulated as a problem separate from artificial intelligence. The propos ed test includes a task that\\ninvolves the automated interpretation and ge neration of  natural langua ge.'), Document(page_content='involves the automated interpretation and ge neration of  natural langua ge.\\nThe premise of symbolic NLP is well-summarized by John Searle\\'s Chinese room experiment: Given a collection of rules\\n(e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural langua ge unde rstanding\\n(or other NLP tasks) by a pplying those rules to the data it confronts.\\n1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian\\nsentences into English. The authors claimed that within three or five years, machine translation would be a\\nsolved problem.[1] However, real progress was much slower, and after the ALPAC report in 1966, which\\nfound that ten-year-long research had failed to fulfill the expectations, funding for machine translation was\\ndramatically reduced. Little further research in machine translation was conducted in America (though\\nsome research continued elsewhere, such as Japan and Europe[2]) until the late 1980s when the first\\nstatistical machine translation systems were developed.\\n1960s: Some notably successful natural language processing systems developed in the 1960s were\\nSHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies,\\nand ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and\\n1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a'), Document(page_content='The earliest decision trees, produc ing systems of hard if–then rules, were still very similar to the old rule-based approaches.\\nOnly the introduc tion of hidden Markov models, applied to part-of-speech tagging, announc ed the end of the old rule-based\\napproach.\\nA major drawback of statistical methods  is that they require elaborate feature engineering. Since 2015,[21] the statistical\\napproach was replaced by neural networks approach, using word embeddings to capture semantic properties of words.\\nIntermediate tasks (e.g., pa rt-of-speech tagging and de pendency parsing) have not been needed anymore.\\nNeural machine translation, based on then-newly-invented sequence-to-sequence transformations, made obsolete the\\nintermediate steps, such as word alignment, previously necessary for statistical machine translation.\\nThe following is a list of some of the most commonly researched tasks in natural langua ge processing. Some of these tasks\\nhave direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger\\ntasks.\\nThough natural langua ge processing tasks are closely intertwined, they can be subdivided into categories for conve nience.\\nA coarse division is given below.\\nOptical character recognition (OCR)\\nGiven an image representing printed text, determine the corresponding text.\\nSpeech recognition\\nGiven a sound clip of a person or people speaking, determine the textual representation of the speech.')]\n"
     ]
    }
   ],
   "source": [
    "question = \"what does natural language processing study?\"\n",
    "docs = vectordb.similarity_search(question,k=3)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Document Q&A "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take the stored document text chunks and the question about the document and pass them both to an LLM. The LLM produces an answer using a prompt that includes the question and the retrieved data.\n",
    "\n",
    "By default we pass all the relevant text chunks in the same call to the LLM. If out chunks of text are too large we can reach the token limit. Here too we can use map reduce to overcome this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RetrievalQA chain performs question answering backed by a retrieval step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Natural language processing (NLP) studies the computational methods and techniques used to enable computers to understand, manipulate, and generate human language. This includes tasks such as speech recognition, natural language understanding, and natural language generation. '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More ressources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Documentation: https://python.langchain.com/docs/get_started/introduction\n",
    "- Really comprehensive tutorials: https://github.com/gkamradt/langchain-tutorials"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
