{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Session Chatbot\n",
    "This is the hands-on session accompanying the workshop on LangChain fundamentals. This is inspired by the more extensive LangChain Cookbook Part 1.\n",
    "\n",
    "Copyright (c) 2023 Michael Neumayr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Set up the Colab in your drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load this Colab from Github\n",
    "- Run the first cell to install all required packages (this takes a moment)\n",
    "- During installation jump to section \"Set OpenAI API Key\" and put the key we provide you instead of \"PUT_YOUR_KEY_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required packages; this may take some minutes; ignore dependency warnings it should work anyway\n",
    "%pip install openai\n",
    "%pip install langchain\n",
    "%pip install pypdf\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the workshop github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/michaelnoi/venture_labs_build.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd venture_labs_build && git checkout only_static_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY', 'PUT_YOUR_KEY_HERE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Interactive Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Remember the list of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"Answer in Chinese.\"),\n",
    "        HumanMessage(content=\"When is the Oktoberfest in Munich usually?\"),\n",
    "        AIMessage(content='The Oktoberfest in Munich usually begins in late September and lasts for 16-18 days, ending on the first Sunday in October or on October 3rd, German Unity Day, if it falls on a Monday.'),\n",
    "        HumanMessage(content=\"And do you have recommendattions what to wear?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a proper interactive chatbot that stores the messages dynamically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Conversation chain for a chatbot\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>The ConversationChain includes</b>\n",
    "    <ol>\n",
    "        <li>a pre-defined prompt template for a conversation with the LLM. The template is filled with the user input and the chat history. See the template below. This saves you the prompt engineering part for a good conversation. If you like more flexibility, however, you can also use your own prompt template.</li>\n",
    "        <li>per default a memory that stores the conversation history and append it to the prompt.</li>\n",
    "    </ol>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "chain = ConversationChain(llm=chat) #, verbose=True)\n",
    "\n",
    "exit_conditions = (\"quit\", \"exit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory is empty at the beginning. The conversation chain will fill it automatically with the conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(chain.memory.buffer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember prompt templates. Here, the user input is automatically prefixed by the \"Human: \" keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up a loop for a conversation with the LLM. The loop should ask the user for input, send the input to the LLM, and print the response. The loop should stop if the user enters \"quit\" or \"exit\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    query = input(\"Human: \")\n",
    "    if query in exit_conditions:\n",
    "        print()\n",
    "        print(\"AI: Goodbye!\")\n",
    "        break\n",
    "    else:\n",
    "        response = chain.predict(input=query)\n",
    "        print()\n",
    "        print(f\"AI: {response}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the memory again:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  <p>Now check the memory of the chain to see if it stored your conversation correctly. You can get a more readible format if you use buffer_as_messages.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: print the stored memory of your conversation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Longer conversations\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Hitting the token limit</b>\n",
    "    <p>Like with handling large documents, you can also hit the token limit with conversations. But that is not just the case when your input is too large but also when the conversation is too long. This happens because you add the whole history to every prompt you make, so the history grows linearly with the number of interactions. This is bad for long conversations and there are multiple ways to fix this.</p>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationSummaryMemory, ConversationBufferWindowMemory\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key)\n",
    "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "chain = ConversationChain(llm=chat) #, verbose=True)\n",
    "\n",
    "exit_conditions = (\"quit\", \"exit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i) Cut the conversation off after a specified limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the simplest version. You can just cut off the conversation after a specified number of interactions. This won't run into any token limit problems but it will also not be able to handle long conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  <p>The keyword <code style=\"color:red\">k</code> is the window size, i.e. how many last interaction will be stored. Play around with k and tell the model something in a conversation and and ask it at some later point again.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "\tllm=chat,\n",
    "\tmemory=ConversationBufferWindowMemory(k=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    query = input(\"Human: \")\n",
    "    if query in exit_conditions:\n",
    "        print()\n",
    "        print(\"AI: Goodbye!\")\n",
    "        break\n",
    "    else:\n",
    "        response = conversation.predict(input=query)\n",
    "        print()\n",
    "        print(f\"AI: {response}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii) Summarize the conversation and append the summary to the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method is to summarize the conversation so far and append that instead of whole raw history to every prompt. This uses more tokens at the beginning but scales better for larger conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  <p>Now utilize <code style=\"color:red\">ConversationSummaryMemory</code> instead of the window memory. You can set it up in the same way, the only difference is, that the summary memory needs an llm (for the summary) as input instead of a window size. The keywork is <code style=\"color:red\">llm=</code>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: set up the conversation chain with a ConversationSummaryMemory here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try it out and print intermediate summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    query = input(\"Human: \")\n",
    "    if query in exit_conditions:\n",
    "        print()\n",
    "        print(\"AI: Goodbye!\")\n",
    "        break\n",
    "    else:\n",
    "        response = conversation.predict(input=query)\n",
    "        print()\n",
    "        print(f\"AI: {response}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii) Compare token usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also combinations of these techniques and more advanced methods, see below how the token usage scales with the number of interactions.\n",
    "\n",
    "<img src=\"static/token_usage.png\" width=\"1000\"/>\n",
    "\n",
    "Source: https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Streaming output, the real ChatGPT experience\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Streaming output</b>\n",
    "    <p>Previously, you always had to wait for the full answer and only then was the result printed. Now, we want to have the real ChatGPT experience and also stream the output token by token as soon at is ready.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same conversation model, we just change how we call the chain. For streaming the outputs, we leverage the <code style=\"color:gray\">streaming</code> keyword. This will return a generator that yields the tokens as they are generated. We can then print them one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], openai_api_key=openai_api_key)\n",
    "chain = ConversationChain(llm=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chat(\n",
    "    [\n",
    "        HumanMessage(content=\"Give me a poem about natural language processing\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Chatbot with streaming output\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "  <p>Put together all the parts from above to create an interactive chatbot that has some memory and streams the output like you know from ChatGPT.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: put your imports here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: put your initializations here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: put your loop here, you don't need to add different arguments to predict for streaming to work as it is already set up in the chat model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More ressources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Documentation: https://python.langchain.com/docs/get_started/introduction\n",
    "- Really comprehensive tutorials: https://github.com/gkamradt/langchain-tutorials\n",
    "- Deep dive conversational memory: https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
